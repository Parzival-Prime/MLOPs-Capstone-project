
# ğŸ§  Sentiment Analysis Pipeline â€“ MLOps Capstone

## ğŸ“Œ Overview

This project is a production-grade Machine Learning pipeline built for sentiment analysis. It serves as a hands-on capstone to master MLOps principles, CI/CD automation, scalable model deployment, and observability practices. The primary goal is to emulate real-world, industry-standard workflows and get deep exposure to technologies like Docker, Kubernetes, and Azure.

---

## ğŸ¯ Objectives

* Implement an end-to-end ML pipeline using MLOps best practices.
* Gain experience in:

  * Data and experiment versioning
  * CI/CD for ML workflows
  * Cloud-native model deployment
  * Monitoring with Prometheus and Grafana

---

## ğŸ§° Tech Stack

| Purpose             | Tool/Technology                    |
| ------------------- | ---------------------------------- |
| Code Versioning     | GitHub                             |
| Data Versioning     | DVC                                |
| Experiment Tracking | MLflow, DagsHub                    |
| Cloud Storage       | Azure Blob Storage                 |
| CI/CD               | GitHub Actions                     |
| Containerization    | Docker (Multi-stage builds)        |
| Image Optimization  | Distroless (experimental)          |
| Orchestration       | Kubernetes (AKS)                   |
| Monitoring          | Prometheus & Grafana               |
| Logging             | Pythonâ€™s built-in `logging` module |

---

## ğŸ“ Project Structure

```plaintext
.
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci_cd_pipeline.yml      # CI/CD definition
â”œâ”€â”€ data/
â”œâ”€â”€ models/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ artifacts/
â”œâ”€â”€ src/
â”‚  â”œâ”€â”€ components/
â”‚     â”œâ”€â”€ data_ingestion.py
â”‚     â”œâ”€â”€ data_transformation.py
â”‚     â”œâ”€â”€ data_validation.py
â”‚     â”œâ”€â”€ feature_engineering.py
â”‚     â”œâ”€â”€ model_training.py
â”‚     â”œâ”€â”€ model_evaluation.py
â”‚     â””â”€â”€ model_pusher.py
â”‚
â”‚   â”œâ”€â”€ configurations/
â”‚     â””â”€â”€ azure_blob_connection.py
â”‚
â”‚   â”œâ”€â”€ constants/
â”‚      â””â”€â”€ constants.py                # Centralized constants
â”‚
â”‚   â”œâ”€â”€ entities/
â”‚      â”œâ”€â”€ config_entity.py
â”‚      â””â”€â”€ artifact_entity.py         # Uses `@dataclass`
â”‚
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ deployments.yaml               # Kubernetes deployment + service
â”œâ”€â”€ dvc.yaml                       # DVC pipeline definition
â”œâ”€â”€ project.toml
â””â”€â”€ setuptools.py
```

---

## âš™ï¸ Functionality Highlights

* **Pipeline Stages**: Modularized and organized under `components/`.
* **Version Control**: All pipeline stages, configs, and outputs tracked with DVC and GitHub.
* **Experiment Tracking**: Seamlessly integrated with MLflow (hosted on DagsHub).
* **Dockerization**: Built multi-stage Dockerfiles to reduce image size. Also explored distroless images.
* **CI/CD**: Automates testing, building, pushing Docker images, and deploying to AKS using GitHub Actions.
* **Monitoring**: Set up Prometheus and Grafana inside the cluster (under a dedicated `monitoring` namespace) to monitor application and cluster health.

---

## ğŸ“ˆ Lessons Learned

* **Logging**: A favorite partâ€”Pythonâ€™s built-in module was extensively used for modular and traceable logging.
* **Distroless Docker**: Explored but found it unnecessarily complex compared to its limited benefits.
* **CI/CD Debugging**: Took around 48 iterations to finalize a robust pipeline.
* **Kubernetes Monitoring**: Learned to configure Prometheus and Grafana on AKS, useful for production-grade observability.

---

## ğŸš€ Getting Started

### Run Locally

```bash
# Clone the repo
git clone <repo-url>
cd <repo-name>

# Create and activate a conda env
conda create -n sentiment-mlops python=3.9 -y
conda activate sentiment-mlops

# Install dependencies
pip install -r requirements.txt

# Reproduce pipeline
dvc repro
```

### Deploy to AKS

Ensure the CI/CD pipeline is correctly configured and push to the `main` branch:

```bash
git add .
git commit -m "Trigger CI/CD"
git push origin main
```

---

## ğŸ“Š Monitoring Dashboard

* Prometheus: Available inside `monitoring` namespace
* Grafana: Default dashboard imported with pod metrics
* Access:

  ```bash
  kubectl port-forward svc/grafana -n monitoring 3000:3000
  ```

---

## âœ… Conclusion

This project reflects a real-world ML production pipeline integrating modern tooling and best practices in data science engineering. It provided a strong foundation for implementing MLOps workflows with reproducibility, traceability, scalability, and observability. (Good Performance was not the aim of this project, though I am going to work on those in near future)

---
